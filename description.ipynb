{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Bell Inequalities from Linear Programming\n",
    "In this repository, I try to explain the usage of linear programming, for finding Bell inequalities, for a given\n",
    "probability distribution. I follow the paper *Bell nonlocality* by *Brunner et. al. (2014)*, which can be found on the [*arXiv*](https://arxiv.org/abs/1303.2849).\n",
    "\n",
    "The connection between Bell inequalities and linear programming interested me, so I though a short python demo would be\n",
    "good to learn about the topic by myself. This repo is just what I've done to understand it better. Maybe it helps someone.\n",
    "\n",
    "## Local behaviors and Bell inequalities\n",
    "\n",
    "Bell inequalities can be used to distinguish between local and non-local probability distributions $p(ab|xy)$.\n",
    "Here $x,y$ are the inputs to the measurement apparatus in a Bell scenario\n",
    "and $a,b$ are the possible outcomes. The set of all possible probabilities $p = \\{p(ab|xy)\\}$ is called the **behavior**.\n",
    "If there are $m$ different choices for the inputs $x,y$ and $\\Delta$ unique outputs for each input, the behavior $p$ has\n",
    "dimension $\\Delta^2 m^2$.\n",
    "\n",
    "Given such a behavior $p$, we want to identify if the correlations are local or not. Therefore we use, that the local\n",
    "polytope $\\mathcal{L}$ and the no-signalling polytope $\\mathcal{NS}$ are both convex. We can then use the *hyperplane\n",
    "seperation method* to see that a hyperplane that seperates a $p \\notin \\mathcal{L}$ from the local polytope $\\mathcal{L}$.\n",
    "\n",
    "If $\\tilde{p} \\notin \\mathcal{L}$, then there exists an inequality\n",
    "\n",
    "$$ s \\cdot p = \\sum_{abxy} s_{xy}^{ab} p(ab|xy) \\leq S_k$$\n",
    "\n",
    "that is fulfilled by all $p \\in \\mathcal{L}$, but not by $\\tilde{p}$. This inequality is called the Bell inequality.\n",
    "\n",
    "### Deterministic representation of local behaviors\n",
    "In the case of a local behavior, a deterministic representation can be found. Therefore a local hidden variable $\\lambda$\n",
    "is used to assign an output to each input:\n",
    "\n",
    "$$ \\lambda = (a_1, ...,a_m; b_1, ..., b_m)$$\n",
    "\n",
    "where $a_1$ is the output that is returned when the input $x=1$ is given. If there are $\\Delta$ different outcomes,\n",
    "we can find that there are $\\Delta^m \\cdot \\Delta^m = \\Delta^{2m}$ possible $\\lambda$.\n",
    "\n",
    "We can define $\\Delta^{2m}$ different local deterministic behaviors $d_\\lambda(ab|xy) \\in \\mathcal{L}$, which can\n",
    "be used to describe **any** local behavior.\n",
    "\n",
    "$$ d_\\lambda(ab|xy) = 1 \\quad \\text{if } a=a_x \\text{ and } b=b_y \\quad\\text{; otherwise } 0$$\n",
    "\n",
    "Any local behavior $p$ may be written in the following form\n",
    "\n",
    "$$ p = \\sum_\\lambda q_\\lambda d_\\lambda \\qquad \\text{ where } \\sum_\\lambda q_\\lambda = 1 \\text{ and } q_\\lambda \\geq 0$$.\n",
    "\n",
    "As all $d_\\lambda$ are known, finding the weights $q_\\lambda$ is a problem of linear programming.\n",
    "\n",
    "### Linear problem\n",
    "The problem of finding the weights can be written as\n",
    "\n",
    "$$ \\text{find } q_\\lambda \\text{ such that}$$ <br>\n",
    "$$ \\sum_\\lambda q_\\lambda d_\\lambda = p \\quad ; \\quad \\sum_\\lambda q_\\lambda = 1 \\quad ; \\quad q_\\lambda \\geq 0$$ <br>\n",
    "\n",
    "However this would not be a optimization problem, but a satisfiability test. To transform it into an optimization problem,\n",
    "we can introduce a **slack variable** $y$. The problem then reads as:\n",
    "\n",
    "$$ \\text{minimize } y \\text{ such that }$$ <br>\n",
    "$$ \\sum_\\lambda q_\\lambda + y= 1 \\quad ; \\quad \\sum_\\lambda q_\\lambda d_\\lambda + y \\cdot p= p$$ <br>\n",
    "$$ q_\\lambda \\geq 0 \\quad ; \\quad y \\geq 0 $$ <br>\n",
    "\n",
    "Now this is the *primal form* of the problem and a trivial solution for $y = 1$ exists. Thus we can not tell if the behavior\n",
    "$p$ was local or not, since the start configuration already satisfies the constraints. However we can look at the **dual\n",
    "form** of the problem. This can be obtained by following some straightforward rules (found e.g. on Wikipedia). The dual\n",
    "form then reads as:\n",
    "\n",
    "$$\\text{maximize } S = s\\cdot p - S_l \\text{ such that}$$ <br>\n",
    "$$ s \\cdot d_\\lambda - S_l \\leq 0 \\quad \\forall \\lambda = 1,..., \\Delta^{2m} $$ <br>\n",
    "$$ s \\cdot p - S_l \\leq 1 $$\n",
    "\n",
    "Here $s \\in \\mathbb{R}^{\\Delta^{2m}}$ and $S_l \\in \\mathbb{R}$. If we now assume that $p$ is local, i.e. can be written\n",
    "as convex combination of local deterministic behaviors $d_\\lambda$, we find that $S \\leq 0$. But we only required that\n",
    "$S \\leq 1$. Thus if $S > 0$ the behavior $p$ is non local. The **Bell inequality** is then:\n",
    "\n",
    "$$ s \\cdot p \\leq S_l $$\n",
    "\n",
    "Thus we can identify any local behavior by this solving this optimization problem. We can also directly say, if a behavior\n",
    "is non-local.\n",
    "\n",
    "### Implementation\n",
    "We formulated the linear problem before and only need to solve it using linear programming. Such an optimizer is included\n",
    "in the *SciPy* package. However we need to rewrite the problem, such that the *SciPy* solver can handle the problem.\n",
    "The implementation of the problem can be found here:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}